bs_64_lr_0.001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Nothing applied to output
    Final Loss: 13.58

bs_64_lr_0.001_run_9:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.67

bs_64_lr_0.0001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.96

bs_64_lr_0.001_run_11:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.25

bs_64_lr_0.001_run_12:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 11.59

bs_64_lr_0.001_run_13:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.33

bs_64_lr_0.001_run_15:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.78

bs_64_lr_0.001_run_16:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.35

bs_64_lr_0.001_run_17:
    4000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.82

bs_64_lr_0.001_run_18:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 8.98

bs_64_lr_0.001_run_21:
    75000 training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.44

bs_64_lr_0.001_run_26:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.6

bs_64_lr_0.001_run_27:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), 128 kernels for layer 5
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 10.47

bs_64_lr_0.001_run_28:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool (after 4)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.49

bs_64_lr_0.001_run_34:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Final Loss: 9.81

bs_64_lr_0.001_run_35:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Sigmoid fc4 and no divide by 100 into tanh
    Final Loss: 47.06

bs_64_lr_0.001_run_36:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    No fc3 or fc4
    Adam
    Final Loss: 1465

bs_64_lr_0.001_run_42:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Notes: Was best so far up to epoch 4 then started to overfit (maybe try with more regularisation)
    Final Loss: 8.7

bs_64_lr_0.001_run_43:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.27

bs_64_lr_0.001_run_44:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.55

bs_64_lr_0.001_run_45:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 9.1

bs_64_lr_0.001_run_46:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.94

bs_64_lr_0.001_run_47:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.25

bs_64_lr_0.001_run_48:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.25

bs_64_lr_0.001_run_49:
    85000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 10

bs_64_lr_0.001_run_51:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 14.8

bs_64_lr_0.001_run_52:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1, after fc2), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 18.5

bs_64_lr_0.001_run_53:
    100000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 15.3

bs_64_lr_0.001_run_54:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10)
    Final Loss: 7.36

bs_64_lr_0.001_run_55:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.2)
    Final Loss: 7.26

bs_64_lr_0.001_run_56:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.5)
    Final Loss: 7.34

bs_64_lr_0.001_run_57:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.5, distortion_scale=0.5)
    Final Loss: 7.02

bs_64_lr_0.001_run_58:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15)
    Final Loss: 6.75

bs_64_lr_0.001_run_59:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.1))
    Final Loss: 6.78

bs_64_lr_0.001_run_60:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 6.05

bs_64_lr_0.001_run_61:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.2))
    Final Loss: 6.223

bs_64_lr_0.001_run_63:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(translate=(0,0.2))
    Final Loss: 6.14

bs_64_lr_0.001_run_64:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=20)
    Final Loss: 6.37

bs_64_lr_0.001_run_67:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    36,48,64 Conv1 - Conv3, 64 Conv4
    Final Loss: 6.24

bs_64_lr_0.001_run_68:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    128 kernels conv5
    Final Loss: 6.77

bs_64_lr_0.001_run_69:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Final Loss: 6.22

bs_64_lr_0.001_run_71:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    36,48,64 Conv1 - Conv3, 64 Conv4, 128 Conv5
    Final Loss: 6.3

bs_64_lr_0.001_run_72:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    (3,3) Conv1-3, (5,5) Conv4-5 (Remember to change padding)
    Final Loss: 6.93

bs_64_lr_0.0001_run_1:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 7.2

bs_64_lr_0.001_run_73:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: y**2 if y > 0 else (2*y)**2 for y in x
    Final Loss: 104.45

bs_64_lr_0.001_run_74:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: y**2 if y > 0 else 2*(y**2) for y in x
    Final Loss: 96.37

bs_64_lr_0.001_run_75:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: 0.5*(y**2) if y > 0 else y**2 for y
    Final Loss: 47.74

bs_64_lr_0.001_run_78:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: mean(y**2 if y > 0 else 2*(y**2) for y in x)
    Final Loss: 6.17

bs_64_lr_0.001_run_79:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: mean(y**2 if y > 0 else (2*y)**2 for y in x)
    Final Loss: 9.14

bs_64_lr_0.001_run_80:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: sum(y**2 if y > 0 else (2*y)**2 for y in x)
    Final Loss: 7.61

bs_64_lr_0.001_run_81:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_loss (-10,10)
    Final Loss (MSE): 6.33

bs_64_lr_0.001_run_82:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_mse (-10,10)
    Final Loss (MSE): 5.98

bs_64_lr_0.001_run_83:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_sum (-10,10)
    Final Loss (MSE): 7.79

bs_64_lr_0.001_run_85:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 5.77

bs_64_lr_0.001_run_90:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_mse (-7.5,7.5)
    Final Loss (MSE): 8.66

bs_64_lr_0.0001_run_10:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.62

bs_64_lr_0.0025_run_0:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Learning rate: 0.0025
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 1465.67

bs_64_lr_0.0001_run_10:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    20 epochs
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.49

bs_64_lr_0.001_run_92:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.015

bs_64_lr_0.001_run_93:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.098

bs_64_lr_0.001_run_94:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9), and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    36,48,64 Conv1-Conv3
    Final Loss (MSE): 5.821

bs_64_lr_0.001_run_115:
    120000 training set
    Original Architecture
    SGD
    Huber
    Normalise images
    Final Loss (MSE):

bs_64_lr_0.001_run_116:
    100000 training set
    BN (1,2,3,4,5), and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Normalise images
    Final Loss (MSE):