bs_64_lr_0.001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Nothing applied to output
    Final Loss: 13.58

bs_64_lr_0.001_run_9:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.67

bs_64_lr_0.0001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.96

bs_64_lr_0.001_run_11:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.25

bs_64_lr_0.001_run_12:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 11.59

bs_64_lr_0.001_run_13:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.33

bs_64_lr_0.001_run_15:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.78

bs_64_lr_0.001_run_16:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.35

bs_64_lr_0.001_run_17:
    4000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.82

bs_64_lr_0.001_run_18:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 8.98

bs_64_lr_0.001_run_21:
    75000 training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.44

bs_64_lr_0.001_run_26:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.6

bs_64_lr_0.001_run_27:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), 128 kernels for layer 5
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 10.47

bs_64_lr_0.001_run_28:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool (after 4)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 8.49

bs_64_lr_0.001_run_34:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Final Loss: 9.81

bs_64_lr_0.001_run_35:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Sigmoid fc4 and no divide by 100 into tanh
    Final Loss: 47.06

bs_64_lr_0.001_run_36:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    No fc3 or fc4
    Final Loss: 1465

bs_64_lr_0.001_run_39:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool (after 2 and 4),
    Stride (1,1) for all conv apart from (2,2) on last
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: