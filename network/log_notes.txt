bs_64_lr_0.001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Nothing applied to output
    Final Loss: 13.58

bs_64_lr_0.001_run_9:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.67

bs_64_lr_0.0001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.96

bs_64_lr_0.001_run_11:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.25

bs_64_lr_0.001_run_12:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 11.59

bs_64_lr_0.001_run_13:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.33

bs_64_lr_0.001_run_15:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.78

bs_64_lr_0.001_run_16:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.35

bs_64_lr_0.001_run_17:
    4000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.82

bs_64_lr_0.001_run_18:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 8.98

bs_64_lr_0.001_run_21:
    75000 training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.44

bs_64_lr_0.001_run_26:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.6

bs_64_lr_0.001_run_27:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), 128 kernels for layer 5
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 10.47

bs_64_lr_0.001_run_28:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool (after 4)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.49

bs_64_lr_0.001_run_34:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Final Loss: 9.81

bs_64_lr_0.001_run_35:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Sigmoid fc4 and no divide by 100 into tanh
    Final Loss: 47.06

bs_64_lr_0.001_run_36:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    No fc3 or fc4
    Adam
    Final Loss: 1465

bs_64_lr_0.001_run_42:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Notes: Was best so far up to epoch 4 then started to overfit (maybe try with more regularisation)
    Final Loss: 8.7

bs_64_lr_0.001_run_43:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.27

bs_64_lr_0.001_run_44:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.55

bs_64_lr_0.001_run_45:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 9.1

bs_64_lr_0.001_run_46:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.94

bs_64_lr_0.001_run_47:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.25

bs_64_lr_0.001_run_48:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.25

bs_64_lr_0.001_run_49:
    85000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 10

bs_64_lr_0.001_run_51:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 14.8

bs_64_lr_0.001_run_52:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1, after fc2), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 18.5

bs_64_lr_0.001_run_53:
    100000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 15.3

bs_64_lr_0.001_run_54:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10)
    Final Loss: 7.36

bs_64_lr_0.001_run_55:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.2)
    Final Loss: 7.26

bs_64_lr_0.001_run_56:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.5)
    Final Loss: 7.34

bs_64_lr_0.001_run_57:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.5, distortion_scale=0.5)
    Final Loss: 7.02

bs_64_lr_0.001_run_58:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15)
    Final Loss: 6.75

bs_64_lr_0.001_run_59:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.1))
    Final Loss: 6.78

bs_64_lr_0.001_run_60:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 6.05

bs_64_lr_0.001_run_61:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.2))
    Final Loss: 6.223

bs_64_lr_0.001_run_63:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(translate=(0,0.2))
    Final Loss: 6.14

bs_64_lr_0.001_run_64:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=20)
    Final Loss: 6.37

bs_64_lr_0.001_run_67:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    36,48,64 Conv1 - Conv3, 64 Conv4
    Final Loss: 6.24

bs_64_lr_0.001_run_68:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    128 kernels conv5
    Final Loss: 6.77

bs_64_lr_0.001_run_69:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Final Loss: 6.22

bs_64_lr_0.001_run_71:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    36,48,64 Conv1 - Conv3, 64 Conv4, 128 Conv5
    Final Loss: 6.3

bs_64_lr_0.001_run_72:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    (3,3) Conv1-3, (5,5) Conv4-5 (Remember to change padding)
    Final Loss: 6.93

bs_64_lr_0.0001_run_1:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 7.2

bs_64_lr_0.001_run_73:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: y**2 if y > 0 else (2*y)**2 for y in x
    Final Loss: 104.45

bs_64_lr_0.001_run_74:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: y**2 if y > 0 else 2*(y**2) for y in x
    Final Loss: 96.37

bs_64_lr_0.001_run_75:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: 0.5*(y**2) if y > 0 else y**2 for y
    Final Loss: 47.74

bs_64_lr_0.001_run_78:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: mean(y**2 if y > 0 else 2*(y**2) for y in x)
    Final Loss: 6.17

bs_64_lr_0.001_run_79:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: mean(y**2 if y > 0 else (2*y)**2 for y in x)
    Final Loss: 9.14

bs_64_lr_0.001_run_80:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: sum(y**2 if y > 0 else (2*y)**2 for y in x)
    Final Loss: 7.61

bs_64_lr_0.001_run_81:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_loss (-10,10)
    Final Loss (MSE): 6.33

bs_64_lr_0.001_run_82:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_mse (-10,10)
    Final Loss (MSE): 5.98

bs_64_lr_0.001_run_83:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_sum (-10,10)
    Final Loss (MSE): 7.79

bs_64_lr_0.001_run_85:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 5.77

bs_64_lr_0.001_run_90:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: truncated_mse (-7.5,7.5)
    Final Loss (MSE): 8.66

bs_64_lr_0.0001_run_10:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.62

bs_64_lr_0.0025_run_0:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Learning rate: 0.0025
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 1465.67

bs_64_lr_0.0001_run_10:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    20 epochs
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.49

bs_64_lr_0.001_run_92:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.015

bs_64_lr_0.001_run_93:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Final Loss (MSE): 6.098

bs_64_lr_0.001_run_94:
    100000 training set
    BN (1,2,3,4,5,6,7,8,9), and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    36,48,64 Conv1-Conv3
    Final Loss (MSE): 5.821

bs_64_lr_0.001_run_115:
    120000 training set
    Original Architecture
    SGD
    Huber
    Normalise images
    Final Loss (MSE):

bs_64_lr_0.001_run_116:
    100000 training set
    BN (1,2,3,4,5), and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Loss: SmoothL1Loss
    Normalise images
    Final Loss (MSE):

Final Architecture Hyper-Parameter Tuning:
    Batch Size:
    - 32: bs_32_lr_0.001_run_2
        Mean error/ground truth: 0.441
        Median error/ground truth: 0.192
        Mean Squared Error: 6.585
        Median L2 Error: 0.893
        Mean L1 Error: 1.700
        Median L1 Error: 0.945
        Loss: 1.271
    - 64:
        Mean error/ground truth: 0.395
        Median error/ground truth: 0.179
        Mean Squared Error: 5.770
        Median L2 Error: 0.795
        Mean L1 Error: 1.571
        Median L1 Error: 0.892
        Loss: 1.163
    - 128: bs_128_lr_0.001_run_1
        Mean error/ground truth: 0.437
        Median error/ground truth: 0.194
        Mean Squared Error: 5.974
        Median L2 Error: 0.996
        Mean L1 Error: 1.656
        Median L1 Error: 0.998
        Loss: 1.233

    Learning Rate:
    - 1e-2:
    - 1e-3:
    - 1e-4: bs_64_lr_0.0001_run_12
        Mean error/ground truth: 0.497
        Median error/ground truth: 0.197
        Mean Squared Error: 6.887
        Median L2 Error: 1.111
        Mean L1 Error: 1.733
        Median L1 Error: 1.054
        Loss: 1.329
    - 5e-3: bs_64_lr_0.005_run_0
    - 5e-4: bs_64_lr_0.0005_run_0
        Mean error/ground truth: 0.478
        Median error/ground truth: 0.186
        Mean Squared Error: 5.969
        Median L2 Error: 0.987
        Mean L1 Error: 1.633
        Median L1 Error: 0.993
        Loss: 1.214

    Weight Decay:

Experiments:
    - ELU instead of ReLU: bs_64_lr_0.001_run_117
        Mean error/ground truth: 0.522
        Median error/ground truth: 0.196
        Mean Squared Error: 7.002
        Median L2 Error: 1.124
        Mean L1 Error: 1.786
        Median L1 Error: 1.060
        Loss: 1.352
    - SGD instead of Adam: bs_64_lr_0.001_run_118
        Mean error/ground truth: 0.639
        Median error/ground truth: 0.229
        Mean Squared Error: 9.179
        Median L2 Error: 1.808
        Mean L1 Error: 2.073
        Median L1 Error: 1.345
        Loss: 1.644
    - No Batch Norm: bs_64_lr_0.001_run_119
        Mean error/ground truth: 0.483
        Median error/ground truth: 0.202
        Mean Squared Error: 7.068
        Median L2 Error: 1.101
        Mean L1 Error: 1.792
        Median L1 Error: 1.049
        Loss: 1.355
    - No Dropout: bs_64_lr_0.001_run_120
        Mean error/ground truth: 0.434
        Median error/ground truth: 0.184
        Mean Squared Error: 5.792
        Median L2 Error: 0.894
        Mean L1 Error: 1.593
        Median L1 Error: 0.946
        Loss: 1.183
    - No Max Pool: bs_64_lr_0.001_run_121
        Mean error/ground truth: 0.428
        Median error/ground truth: 0.195
        Mean Squared Error: 6.719
        Median L2 Error: 0.973
        Mean L1 Error: 1.681
        Median L1 Error: 0.986
        Loss: 1.281
    - No tanh: bs_64_lr_0.001_run_122
        Mean error/ground truth: 0.544
        Median error/ground truth: 0.195
        Mean Squared Error: 6.431
        Median L2 Error: 1.133
        Mean L1 Error: 1.757
        Median L1 Error: 1.065
        Loss: 1.318
    - No Data Augmentation: bs_64_lr_0.001_run_124
    - AMSGrad: bs_64_lr_0.001_run_125