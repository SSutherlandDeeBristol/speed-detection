bs_64_lr_0.001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Nothing applied to output
    Final Loss: 13.58

bs_64_lr_0.001_run_9:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.67

bs_64_lr_0.0001_run_0:
    3000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.96

bs_64_lr_0.001_run_11:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    Final Loss: 14.25

bs_64_lr_0.001_run_12:
    3000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 11.59

bs_64_lr_0.001_run_13:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    Final Loss: 13.33

bs_64_lr_0.001_run_15:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 3, after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.78

bs_64_lr_0.001_run_16:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.35

bs_64_lr_0.001_run_17:
    4000 vid training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.82

bs_64_lr_0.001_run_18:
    4000 vid training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 8.98

bs_64_lr_0.001_run_21:
    75000 training set
    BN (1,2,3,4,5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.44

bs_64_lr_0.001_run_26:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 9.6

bs_64_lr_0.001_run_27:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), 128 kernels for layer 5
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Final Loss: 10.47

bs_64_lr_0.001_run_28:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool (after 4)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.49

bs_64_lr_0.001_run_34:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Final Loss: 9.81

bs_64_lr_0.001_run_35:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    AdamW
    Sigmoid fc4 and no divide by 100 into tanh
    Final Loss: 47.06

bs_64_lr_0.001_run_36:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5)
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    No fc3 or fc4
    Adam
    Final Loss: 1465

bs_64_lr_0.001_run_42:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Notes: Was best so far up to epoch 4 then started to overfit (maybe try with more regularisation)
    Final Loss: 8.7

bs_64_lr_0.001_run_43:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.27

bs_64_lr_0.001_run_44:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.55

bs_64_lr_0.001_run_45:
    75000 training set
    BN (1,2,3,4,5) and Dropout (after 3 and 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 9.1

bs_64_lr_0.001_run_46:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.94

bs_64_lr_0.001_run_47:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 7.25

bs_64_lr_0.001_run_48:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 8.25

bs_64_lr_0.001_run_49:
    85000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 2
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 10

bs_64_lr_0.001_run_51:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 14.8

bs_64_lr_0.001_run_52:
    85000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1, after fc2), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 18.5

bs_64_lr_0.001_run_53:
    100000 training set but with fewer zeros
    BN (1,2,3,4,5) and Dropout (after 5, after fc1), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    Final Loss: 15.3

bs_64_lr_0.001_run_54:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10)
    Final Loss: 7.36

bs_64_lr_0.001_run_55:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.2)
    Final Loss: 7.26

bs_64_lr_0.001_run_56:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.2, distortion_scale=0.5)
    Final Loss: 7.34

bs_64_lr_0.001_run_57:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomPerspective(p=0.5, distortion_scale=0.5)
    Final Loss: 7.02

bs_64_lr_0.001_run_58:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15)
    Final Loss: 6.75

bs_64_lr_0.001_run_59:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.1))
    Final Loss: 6.78

bs_64_lr_0.001_run_60:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 6.05

bs_64_lr_0.001_run_61:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=10, translate=(0,0.2))
    Final Loss: 6.223

bs_64_lr_0.001_run_63:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(translate=(0,0.2))
    Final Loss: 6.14

bs_64_lr_0.001_run_64:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=20)
    Final Loss: 6.37

bs_64_lr_0.001_run_67:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    36,48,64 Conv1 - Conv3, 64 Conv4
    Final Loss: 6.24

bs_64_lr_0.001_run_68:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    128 kernels conv5
    Final Loss: 6.77

bs_64_lr_0.001_run_69:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    Final Loss: 6.22

bs_64_lr_0.001_run_71:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.2))
    36,48,64 Conv1 - Conv3, 64 Conv4, 128 Conv5
    Final Loss: 6.3

bs_64_lr_0.001_run_72:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    (3,3) Conv1-3, (5,5) Conv4-5 (Remember to change padding)
    Final Loss: 6.93

bs_64_lr_0.0001_run_1:
    100000 training set
    BN (1,2,3,4,5) and Dropout (after 5), Max-Pool after 3
    Output put through tanh(x / 100) * 45
    ReLU instead of ELU
    Adam
    RandomAffine(degrees=15, translate=(0,0.1))
    Final Loss: 7.2